{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "import os\n",
    "import threading\n",
    "import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from torch_geometric.loader.link_neighbor_loader import LinkNeighborLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import (\n",
    "                                    HeteroData,\n",
    "                                    Data, \n",
    "                                    Batch\n",
    "                                 )   \n",
    "from torch_geometric.nn import (\n",
    "                                GATv2Conv,\n",
    "                                SAGPooling,\n",
    "                                global_add_pool,\n",
    "                                HeteroConv,\n",
    "                                Linear,\n",
    "                                to_hetero\n",
    "                                )\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, KFold, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score, \n",
    "    precision_recall_curve, \n",
    "    auc, \n",
    "    average_precision_score, \n",
    "    matthews_corrcoef\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed all randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage example:\n",
    "seed_everything(42)  # Set the seed to 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = data.to_dict()\n",
    "fnm = '../prep_data/hetero_graph/hetero_data_dict.pt'\n",
    "data = torch.load(fnm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={ node_id=[1007] },\n",
       "  \u001b[1mside_effect\u001b[0m={ node_id=[5587] },\n",
       "  \u001b[1m(drug, known, side_effect)\u001b[0m={ edge_index=[2, 132063] },\n",
       "  \u001b[1m(drug, struct, drug)\u001b[0m={\n",
       "    edge_index=[2, 15844],\n",
       "    edge_attr=[15844]\n",
       "  },\n",
       "  \u001b[1m(drug, word, drug)\u001b[0m={\n",
       "    edge_index=[2, 83865],\n",
       "    edge_attr=[83865]\n",
       "  },\n",
       "  \u001b[1m(drug, target, drug)\u001b[0m={\n",
       "    edge_index=[2, 3363],\n",
       "    edge_attr=[3363]\n",
       "  },\n",
       "  \u001b[1m(drug, se_encoded, drug)\u001b[0m={\n",
       "    edge_index=[2, 65854],\n",
       "    edge_attr=[65854]\n",
       "  },\n",
       "  \u001b[1m(side_effect, name, side_effect)\u001b[0m={\n",
       "    edge_index=[2, 299170],\n",
       "    edge_attr=[299170]\n",
       "  },\n",
       "  \u001b[1m(side_effect, dg_encoded, side_effect)\u001b[0m={\n",
       "    edge_index=[2, 101114],\n",
       "    edge_attr=[101114]\n",
       "  },\n",
       "  \u001b[1m(side_effect, atc, side_effect)\u001b[0m={\n",
       "    edge_index=[2, 26140],\n",
       "    edge_attr=[26140]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transformation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_TO_ID_DICT = {}\n",
    "drug_id_mol_graph_tup = []\n",
    "ID_TO_DB_DICT = {}\n",
    "MEDRAID_TO_ID_DICT = {}\n",
    "ID_TO_MEDRAID_DICT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = [DB_TO_ID_DICT, ID_TO_DB_DICT, MEDRAID_TO_ID_DICT, ID_TO_MEDRAID_DICT, drug_id_mol_graph_tup]\n",
    "file_names = ['db_to_id.pt', 'id_to_db.pt', 'uml_to_id.pt', 'id_to_uml.pt', 'drug_to_mol.pt']\n",
    "\n",
    "for data_dict, fnm in zip(dict_list, file_names):\n",
    "    full_path = f\"../prep_data/hetero_graph/{fnm}\"\n",
    "    loaded_data = torch.load(full_path)\n",
    "    \n",
    "    if isinstance(data_dict, dict):\n",
    "        data_dict.update(loaded_data)\n",
    "    elif isinstance(data_dict, list):\n",
    "        data_dict.extend(loaded_data)\n",
    "    else:\n",
    "        # If it's neither a dict nor a list, just replace it\n",
    "        index = dict_list.index(data_dict)\n",
    "        dict_list[index] = loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant Simple - Homogenous GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_similarity_edges = [('drug', 'struct', 'drug'),\n",
    "                           ('drug', 'word', 'drug'),\n",
    "                           ('drug', 'target', 'drug'),\n",
    "                           ('drug', 'se_encoded', 'drug'),\n",
    "                           ('side_effect', 'name', 'side_effect'),\n",
    "                           ('side_effect', 'dg_encoded', 'side_effect'),\n",
    "                           ('side_effect', 'atc', 'side_effect')\n",
    "                            ]\n",
    "for edge in remove_similarity_edges:\n",
    "    del data[edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={ node_id=[1007] },\n",
       "  \u001b[1mside_effect\u001b[0m={ node_id=[5587] },\n",
       "  \u001b[1m(drug, known, side_effect)\u001b[0m={ edge_index=[2, 132063] }\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeteroData Undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initalized node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_drug_nodes = data['drug']['node_id'].size()[0]\n",
    "num_se_nodes = data['side_effect']['node_id'].size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7288, 0.7229, 0.2304,  ..., 0.4231, 0.8584, 0.4674],\n",
       "        [0.1594, 0.9352, 0.9086,  ..., 0.6596, 0.9826, 0.5885],\n",
       "        [0.6754, 0.1170, 0.0593,  ..., 0.5903, 0.3413, 0.2729],\n",
       "        ...,\n",
       "        [0.8108, 0.0930, 0.7023,  ..., 0.3677, 0.7749, 0.9355],\n",
       "        [0.8262, 0.4098, 0.4337,  ..., 0.0729, 0.7103, 0.2575],\n",
       "        [0.0270, 0.8912, 0.3906,  ..., 0.7522, 0.7644, 0.2061]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize feature tensors\n",
    "drug_features = torch.empty(num_drug_nodes, feature_dim)\n",
    "side_effect_features = torch.empty(num_se_nodes, feature_dim)\n",
    "\n",
    "# Xavier Uniform Initialization with different parameters\n",
    "nn.init.uniform_(drug_features)\n",
    "nn.init.uniform_(side_effect_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogenous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mdrug\u001b[0m={ node_id=[1007] },\n",
       "  \u001b[1mside_effect\u001b[0m={ node_id=[5587] },\n",
       "  \u001b[1m(drug, known, side_effect)\u001b[0m={ edge_index=[2, 132063] }\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 132063], node_id=[6594], node_type=[6594], edge_type=[132063])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_data = data.to_homogeneous()\n",
    "homo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_data.node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Manually combine node features\n",
    "all_features = torch.cat([drug_features, side_effect_features], dim=0)\n",
    "\n",
    "# Step 3: Assign the combined features to the homogeneous data\n",
    "homo_data.x = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 132063], node_id=[6594], node_type=[6594], edge_type=[132063], x=[6594, 384])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_data(data, k=10, shuffle=True, num_neighbors=[10, 4], batch_size=64):\n",
    "    kf = KFold(n_splits=k, shuffle=shuffle)\n",
    "    kf.get_n_splits()\n",
    "    train_val_data_X = data.edge_index.T.numpy()\n",
    "    for train_index, test_index in kf.split(train_val_data_X):\n",
    "        train_index_, valid_index_ = train_test_split(train_index, test_size=0.1)\n",
    "        train_set = train_index_\n",
    "        valid_set = valid_index_\n",
    "        \n",
    "        train_data_cv = copy.deepcopy(data)\n",
    "        train_data_cv.edge_index = torch.tensor(train_val_data_X[train_set].T)\n",
    "        \n",
    "    \n",
    "        val_data_cv = copy.deepcopy(data)\n",
    "        val_data_cv.edge_index = torch.tensor(train_val_data_X[valid_set].T)\n",
    "        \n",
    "        \n",
    "        test_data_cv = copy.deepcopy(data)\n",
    "        test_data_cv.edge_index = torch.tensor(train_val_data_X[test_index].T)\n",
    "    \n",
    "        \n",
    "        # use RandomLinkSplit to get disjoint train ratio an other pyg transforms\n",
    "        transform = T.RandomLinkSplit(\n",
    "            num_val=0.0,\n",
    "            num_test=0.0,\n",
    "            disjoint_train_ratio=0.3236238313900354,\n",
    "            neg_sampling_ratio=0.0,\n",
    "            add_negative_train_samples=False,\n",
    "            is_undirected=True,\n",
    "        )\n",
    "        train_cv, _, _ = transform(train_data_cv)\n",
    "        \n",
    "        transform = T.RandomLinkSplit(\n",
    "            num_val=0.0,\n",
    "            num_test=0.0,\n",
    "            disjoint_train_ratio=0.99,\n",
    "            neg_sampling_ratio=1.0,\n",
    "            add_negative_train_samples=True,\n",
    "            is_undirected=True\n",
    "        )\n",
    "        \n",
    "        val_cv, _, _ = transform(val_data_cv)\n",
    "       \n",
    "\n",
    "        test_cv, _, _ = transform(test_data_cv)\n",
    "        # Define seed edges:\n",
    "        edge_label_index = train_cv.edge_label_index\n",
    "        edge_label = train_cv.edge_label\n",
    "\n",
    "        train_loader = LinkNeighborLoader(\n",
    "            data=train_cv,\n",
    "            num_neighbors=num_neighbors,\n",
    "            neg_sampling_ratio=1.0,\n",
    "            edge_label_index=edge_label_index,\n",
    "            edge_label=edge_label,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            # disjoint=True,\n",
    "        )\n",
    "        \n",
    "        edge_label_index = val_cv.edge_label_index\n",
    "        edge_label = val_cv.edge_label\n",
    "        # num_neighbors is a dictionary, it uses the specified number for each edge type\n",
    "        val_loader = LinkNeighborLoader(\n",
    "            data=val_cv,\n",
    "            num_neighbors=num_neighbors,\n",
    "            edge_label_index=edge_label_index,\n",
    "            edge_label=edge_label,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        edge_label_index = test_cv.edge_label_index\n",
    "        edge_label = test_cv.edge_label\n",
    "\n",
    "        test_loader = LinkNeighborLoader(\n",
    "            data=test_cv,\n",
    "            num_neighbors=num_neighbors,\n",
    "            edge_label_index= edge_label_index,\n",
    "            edge_label=edge_label,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        yield train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroMHGNN(nn.Module):\n",
    "    def __init__(self, metadata, in_channels, hidden_dims, heads, use_edge_attr=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleDict()\n",
    "        self.skips = nn.ModuleDict()\n",
    "        self.final_norms = nn.ModuleDict()\n",
    "        \n",
    "        # Define which edge types should use edge attributes\n",
    "        if use_edge_attr is None:\n",
    "            use_edge_attr = {edge_type: False for edge_type in metadata[1]}\n",
    "        \n",
    "        for i, (out_dim, head) in enumerate(zip(hidden_dims, heads)):\n",
    "            conv_dict = {}\n",
    "            for edge_type in metadata[1]:\n",
    "                src, _, dst = edge_type\n",
    "                if i == 0:\n",
    "                    in_channels = in_channels\n",
    "                else:\n",
    "                    in_channels = hidden_dims[i-1] * heads[i-1]\n",
    "                \n",
    "                if use_edge_attr[edge_type]:\n",
    "                    conv_dict[edge_type] = GATv2Conv(in_channels, out_dim, heads=head, add_self_loops=False, edge_dim=1)\n",
    "                else:\n",
    "                    conv_dict[edge_type] = GATv2Conv(in_channels, out_dim, heads=head, add_self_loops=False)\n",
    "            \n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='sum'))\n",
    "            \n",
    "            for node_type in metadata[0]:\n",
    "                self.norms[f'{node_type}_{i}'] = nn.LayerNorm(out_dim * head)\n",
    "                if i == 0:\n",
    "                    self.skips[f'{node_type}_{i}'] = Linear(in_channels, out_dim * head)\n",
    "                else:\n",
    "                    self.skips[f'{node_type}_{i}'] = Linear(hidden_dims[i-1] * heads[i-1], out_dim * head)\n",
    "        \n",
    "        self.node_types = metadata[0]\n",
    "        for node_type in metadata[0]:\n",
    "            self.final_norms[f'{node_type}'] = nn.LayerNorm(out_dim * head *len(heads))\n",
    "        \n",
    "        # Initialize skips with xavier init\n",
    "        for skip in self.skips.values():\n",
    "            nn.init.xavier_uniform_(skip.weight)\n",
    "            nn.init.zeros_(skip.bias)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        x_repr_dict = {node_type: [] for node_type in self.node_types}\n",
    "        # edge_attr_dict = {key: value.to(torch.float32) for key, value in edge_attr_dict.items()}\n",
    "\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            skip_x = {}\n",
    "            for node_type in self.node_types:\n",
    "                skip_x[node_type] = self.skips[f'{node_type}_{i}'](x_dict[node_type])\n",
    "            \n",
    "            x_dict_new = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "            \n",
    "            for node_type in self.node_types:\n",
    "                # skip_x = self.skips[f'{node_type}_{i}'](x_dict[node_type])\n",
    "                x = x_dict_new[node_type]\n",
    "                x = self.norms[f'{node_type}_{i}'](x) + skip_x[node_type]\n",
    "                x = self.norms[f'{node_type}_{i}'](x)\n",
    "                x = F.elu(x)\n",
    "                x_repr_dict[node_type].append(x)\n",
    "                x_dict[node_type] = x\n",
    "        \n",
    "        # Concatenate all representations for each node type\n",
    "        for node_type in self.node_types:\n",
    "            x_repr_dict[node_type] = self.final_norms[f'{node_type}'](torch.cat(x_repr_dict[node_type], dim=1))\n",
    "        \n",
    "        return x_repr_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final classifier applies the hammard-product between source and destination\n",
    "# node embeddings to derive edge-level predictions:\n",
    "class VanillaHomoClassifier(torch.nn.Module):\n",
    "    def forward(self, x: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_drug = x[edge_label_index[0]]\n",
    "        edge_feat_se = x[edge_label_index[1]]\n",
    "\n",
    "        # Apply hammard-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_drug * edge_feat_se).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FinalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomoModel(torch.nn.Module):\n",
    "    def __init__(self, gnn_model, classifier_model, num_nodes):\n",
    "        super().__init__()\n",
    "        self.inital_norm = nn.LayerNorm(384)\n",
    "        # self.node_emb = torch.nn.Embedding(num_nodes, 384)\n",
    "        \n",
    "        self.gnn = gnn_model \n",
    "        # Instantiate classifier:\n",
    "        self.classifier = classifier_model   \n",
    "    \n",
    "    def forward(self, data: Data) -> Tensor:\n",
    "        x = self.inital_norm(data.x)\n",
    "        x_dict = {\"node\": x}\n",
    "        edge_index_dict = {('node', 'edge', 'node'):data.edge_index}\n",
    "        edge_index_attr = {('node', 'edge', 'node'):data.edge_attr}\n",
    "        x = self.gnn(x_dict, edge_index_dict, edge_index_attr)\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"node\"],\n",
    "            data.edge_label_index,\n",
    "        )\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_compute(batch, device, model):\n",
    "    # batch = batch.to(device)\n",
    "    pred = model(batch)\n",
    "    actual = batch.edge_label\n",
    "    return pred, actual\n",
    "\n",
    "# def do_train_compute(batch, device, model):\n",
    "#     # batch = batch.to(device)\n",
    "#     pred = model(batch)\n",
    "#     actual = batch.edge_label\n",
    "#     return pred, actual\n",
    "\n",
    "\n",
    "def evaluate_metrics(probas_pred, ground_truth):\n",
    "    # compute binary classification metrics using sklearn\n",
    "    # convert to numpy array\n",
    "    probas_pred = probas_pred.numpy()\n",
    "    \n",
    "    ground_truth = ground_truth.numpy()\n",
    "    \n",
    "    # convert to binary predictions\n",
    "    binary_pred = np.where(probas_pred > 0.5, 1, 0)\n",
    "\n",
    "    \n",
    "    # compute metrics\n",
    "    accuracy = accuracy_score(ground_truth, binary_pred)\n",
    "    precision = precision_score(ground_truth, binary_pred)\n",
    "    recall = recall_score(ground_truth, binary_pred)\n",
    "    f1 = f1_score(ground_truth, binary_pred)\n",
    "    roc_auc = roc_auc_score(ground_truth, probas_pred)\n",
    "    precision_, recall_, _ = precision_recall_curve(ground_truth, probas_pred)\n",
    "    pr_auc = auc(recall_, precision_)\n",
    "    average_precision = average_precision_score(ground_truth, probas_pred)\n",
    "    return accuracy, precision, recall, f1, roc_auc, pr_auc, average_precision\n",
    "\n",
    "def train_loop(model, model_name, writer, train_loader, val_loader, loss_fn, optimizer, n_epochs, device, scheduler=None, early_stopping_patience=3, early_stopping_counter=0):\n",
    "    early_stop = False\n",
    "    best_val_metrics = -float(\"inf\") #-float(\"inf\")\n",
    "    best_model_path = f\"saved_models/{model_name}/best_model.pth\"\n",
    "    # make best_model_path parent directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "    \n",
    "    print(\"Starting training loop at\", datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    total_train_val_steps = len(train_loader) + len(val_loader)\n",
    "    epoch_progress_bar = tqdm.notebook.tqdm(range(1, (total_train_val_steps*n_epochs)+1), desc=\"MiniBatches\")\n",
    "    epoch = 0\n",
    "    for _ in epoch_progress_bar:\n",
    "        epoch += 1\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_probas_pred = []\n",
    "        train_ground_truth = []\n",
    "        val_probas_pred = []\n",
    "        val_ground_truth = []\n",
    "        print(\"Epoch\", epoch)\n",
    "        \n",
    "        model.train()\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            optimizer.zero_grad()\n",
    "            out, actual = do_train_compute(batch, device, model)\n",
    "            pred = torch.sigmoid(out)\n",
    "            train_probas_pred.append(pred.detach().cpu())\n",
    "            train_ground_truth.append(actual.detach().cpu())\n",
    "            loss = loss_fn(out, actual)\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            epoch_progress_bar.set_postfix_str(f\"Epoch {epoch} - LR {lr:.7f} - Train Batch {idx+1}/{len(train_loader)} - Train loss: {train_loss/(idx+1):.4f}\")\n",
    "            epoch_progress_bar.update()\n",
    "            writer.add_scalar(\"Training Loss MiniBatch\", loss.item(), idx)\n",
    "            batch = batch.to(\"cpu\")\n",
    "            # if scheduler is not None: # cosine annealing scheduler\n",
    "            #     scheduler.step()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        writer.add_scalar(\"Training Loss Epoch\", train_loss, epoch)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_probas_pred = torch.cat(train_probas_pred, dim=0)\n",
    "            train_ground_truth = torch.cat(train_ground_truth, dim=0)\n",
    "            train_accuracy, train_precision, train_recall, train_f1, \\\n",
    "                train_roc_auc, train_pr_auc, train_average_precision = evaluate_metrics(train_probas_pred, train_ground_truth)\n",
    "            writer.add_scalar(\"Training Accuracy\", train_accuracy, epoch)\n",
    "            writer.add_scalar(\"Training Precision\", train_precision, epoch)\n",
    "            writer.add_scalar(\"Training Recall\", train_recall, epoch)\n",
    "            writer.add_scalar(\"Training F1\", train_f1, epoch)\n",
    "            writer.add_scalar(\"Training ROC AUC\", train_roc_auc, epoch)\n",
    "            writer.add_scalar(\"Training PR AUC\", train_pr_auc, epoch)\n",
    "            writer.add_scalar(\"Training Average Precision\", train_average_precision, epoch)\n",
    "\n",
    "            for idx_, batch in enumerate(val_loader):\n",
    "                batch = batch.to(device)\n",
    "                out, actual = do_train_compute(batch, device, model)\n",
    "                pred = torch.sigmoid(out)\n",
    "                val_probas_pred.append(pred.detach().cpu())\n",
    "                val_ground_truth.append(actual.detach().cpu())\n",
    "                loss = loss_fn(out, actual)\n",
    "                val_loss += loss.item()\n",
    "                epoch_progress_bar.set_postfix_str(f\"Epoch {epoch} - LR {lr:.7f} - Val Batch {idx_+1}/{len(val_loader)} - Val loss: {val_loss/(idx+1):.4f}\")\n",
    "                epoch_progress_bar.update()\n",
    "                writer.add_scalar(\"Validation Loss MiniBatch\", loss.item(), idx_)\n",
    "                batch = batch.to(\"cpu\")\n",
    "            val_loss /= len(val_loader)\n",
    "            val_probas_pred = torch.cat(val_probas_pred, dim=0)\n",
    "            val_ground_truth = torch.cat(val_ground_truth, dim=0)\n",
    "            val_accuracy, val_precision, val_recall, val_f1, \\\n",
    "                val_roc_auc, val_pr_auc, val_average_precision = evaluate_metrics(val_probas_pred, val_ground_truth)\n",
    "            \n",
    "            writer.add_scalar(\"Validation Loss Epoch\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Validation Accuracy\", val_accuracy, epoch)\n",
    "            writer.add_scalar(\"Validation Precision\", val_precision, epoch)\n",
    "            writer.add_scalar(\"Validation Recall\", val_recall, epoch)\n",
    "            writer.add_scalar(\"Validation F1\", val_f1, epoch)\n",
    "            writer.add_scalar(\"Validation ROC AUC\", val_roc_auc, epoch)\n",
    "            writer.add_scalar(\"Validation PR AUC\", val_pr_auc, epoch)\n",
    "            writer.add_scalar(\"Validation Average Precision\", val_average_precision, epoch)\n",
    "            \n",
    "            if val_f1 > best_val_metrics:\n",
    "                best_val_metrics = val_f1\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(\"New best model saved!\") \n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(\"Early stopping counter:\", early_stopping_counter)\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    early_stop = True\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_f1) #\n",
    "      \n",
    "        \n",
    "        epoch_progress_bar.set_postfix_str(\"Train loss: {:.4f}, Train f1: {:.4f}, Train auc: {:.4f}, Train pr_auc: {:.4f},\\\n",
    "                                            Val loss: {:.4f}, Val f1: {:.4f}, Val auc: {:.4f}, Val pr_auc: {:.4f},\\\n",
    "                                            Best val f1: {:.4f}\".format(train_loss, train_f1, train_roc_auc, train_pr_auc,\\\n",
    "                                            val_loss, val_f1, val_roc_auc, val_pr_auc, best_val_metrics))\n",
    "        epoch_progress_bar.update()\n",
    "        print(\"Epoch Number:\", epoch)   \n",
    "        print(\"Epoch time:\", time.time() - start_time)\n",
    "        print(\"Train loss:\", train_loss)\n",
    "        print(\"Train accuracy:\", train_accuracy)\n",
    "        print(\"Train precision:\", train_precision)\n",
    "        print(\"Train recall:\", train_recall)\n",
    "        print(\"Train f1:\", train_f1)\n",
    "        print(\"Train roc_auc:\", train_roc_auc)\n",
    "        print(\"Train pr_auc:\", train_pr_auc)\n",
    "        print(\"Train average_precision:\", train_average_precision)\n",
    "        \n",
    "        print(\"Val loss:\", val_loss)\n",
    "        print(\"Val accuracy:\", val_accuracy)\n",
    "        print(\"Val precision:\", val_precision)\n",
    "        print(\"Val recall:\", val_recall)\n",
    "        print(\"Val f1:\", val_f1)\n",
    "        print(\"Val roc_auc:\", val_roc_auc)\n",
    "        print(\"Val pr_auc:\", val_pr_auc)\n",
    "        print(\"Val average_precision:\", val_average_precision)\n",
    "        print(\"Best val_f1:\", best_val_metrics)\n",
    "        print()\n",
    "        if early_stop:\n",
    "            break\n",
    "        if epoch == n_epochs:\n",
    "            print(\"Training completed!\")\n",
    "            break\n",
    "    \n",
    "    # load best model \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrank(y, y_pre):\n",
    "    index = np.argsort(-y_pre)\n",
    "    r_label = y[index]\n",
    "    r_index = np.array(np.where(r_label == 1)) + 1\n",
    "    reci_sum = np.sum(1 / r_index)\n",
    "    reci_rank = np.mean(1 / r_index)\n",
    "    return reci_sum\n",
    "\n",
    "def evaluate_fold(loader, model, device, ret=False):\n",
    "    preds = []\n",
    "    ground_truths = []\n",
    "    model.eval()\n",
    "    for sampled_data in tqdm.tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            sampled_data.to(device)\n",
    "            pred = model(sampled_data) \n",
    "            # Applying sigmoid activation function to the predicted values\n",
    "            output_probs = torch.sigmoid(pred)\n",
    "\n",
    "            preds.append(output_probs)\n",
    "            ground_truths.append(sampled_data.edge_label)\n",
    "\n",
    "    pred = torch.cat(preds, dim=0).cpu().numpy()\n",
    "    pred_int = (pred>0.5).astype(int)\n",
    "    ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
    "\n",
    "    auc = roc_auc_score(ground_truth, pred)\n",
    "    ap = average_precision_score(ground_truth, pred)\n",
    "    mr = mrank(ground_truth, pred)\n",
    "    f1 = f1_score(ground_truth, pred_int)\n",
    "    mcc = matthews_corrcoef(ground_truth, pred_int)\n",
    "    acc = (pred_int == ground_truth).mean()\n",
    "    precision = precision_score(ground_truth, pred_int)\n",
    "    recall = recall_score(ground_truth, pred_int)\n",
    "    print()\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "    print(f\"Test AP: {ap:.4f}\")\n",
    "    print(f\"Test F1: {f1:.4f}\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Test Precission: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test MCC: {mcc:.4f}\")\n",
    "    print(f\"Test MR: {mr:.4f}\")\n",
    "    if ret:\n",
    "        return auc, ap, f1, acc, precision, recall, mr, mcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Wrap CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 132063], node_id=[6594], node_type=[6594], edge_type=[132063], x=[6594, 384])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrap_cv(data, model_name_, cv_fold=10, shuffle=True, num_neighbors=[10, 4], batch_size=64, n_epochs=10, early_stopping_patience=5):\n",
    "    eval_metrics = []\n",
    "    for i, (train_loader_cv, val_loader_cv,  test_loader_cv) in enumerate(get_kfold_data(data, k=cv_fold,\n",
    "                                                                shuffle=shuffle,\n",
    "                                                                num_neighbors=num_neighbors, \n",
    "                                                                batch_size=batch_size)):\n",
    "        print(f\"Fold {i+1}\")\n",
    "        model_name = f\"{model_name_}/fold{i+1}\"\n",
    "        # Define the log directory where TensorBoard logs will be stored\n",
    "        log_dir = f\"logs/{model_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Create a SummaryWriter\n",
    "        writer = SummaryWriter(log_dir)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Device: '{device}'\")\n",
    "\n",
    "        # gnn_model = MHGNN(input_dim=384, hidden_dims=[64, 64, 64], heads=[2, 2, 2])\n",
    "\n",
    "        # gnn_model = to_hetero(gnn_model, metadata=data.metadata())\n",
    "        use_edge_attr = {('node', 'edge', 'node'): False}\n",
    "        homo_metadata = (['node'], [('node', 'edge', 'node')])\n",
    "        gnn_model = HeteroMHGNN(homo_metadata, in_channels=384, hidden_dims=[64, 64, 64], heads=[2, 2, 2], use_edge_attr=use_edge_attr)\n",
    "        classifier_model = VanillaHomoClassifier()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = HomoModel(gnn_model=gnn_model,\n",
    "                        classifier_model=classifier_model, num_nodes=6594)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00025062971034390006,  weight_decay=0.001)\n",
    "        \n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=0.00025062971034390006, weight_decay=0.001)\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "        #                                                                  T_0=len(train_loader_cv), \n",
    "        #                                                                  T_mult=1, eta_min=1e-5, \n",
    "        #                                                                  verbose=False)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"max\", factor=0.5, patience=2, min_lr=1e-6\n",
    "            )\n",
    "        # scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))\n",
    "    \n",
    "        print(f\"Total Number of Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        print(f\"Total Number of Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "        \n",
    "        model = train_loop(model, model_name, writer, train_loader_cv, val_loader_cv,\n",
    "                             F.binary_cross_entropy_with_logits, optimizer, n_epochs=n_epochs, \n",
    "                             device=device, scheduler=scheduler, early_stopping_patience=early_stopping_patience)\n",
    "\n",
    "        # load best model and store evaluation metrics\n",
    "        model.load_state_dict(torch.load(f'saved_models/{model_name}/best_model.pth'))\n",
    "        auc, ap, f1, acc, precision, recall, mr, mcc = evaluate_fold(test_loader_cv, model, torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), ret=True)\n",
    "        eval_metrics.append([auc, ap, f1, acc, precision, recall, mr, mcc])\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Train CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf logs/* saved_models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 16:12:47.433748: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 16:12:47.547455: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-05 16:12:47.991116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-05 16:12:47.991396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-05 16:12:47.991401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 'cuda'\n",
      "Total Number of Parameters: 249984\n",
      "Total Number of Trainable Parameters: 249984\n",
      "Starting training loop at 2024-08-05 16:12:50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353dec9b99bd4c2c82035e3b47415890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MiniBatches:   0%|          | 0/6740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "New best model saved!\n",
      "Epoch Number: 1\n",
      "Epoch time: 11.21910834312439\n",
      "Train loss: 2.7331895480429442\n",
      "Train accuracy: 0.6133686919897043\n",
      "Train precision: 0.577590348582715\n",
      "Train recall: 0.8439279307386319\n",
      "Train f1: 0.6858084553463903\n",
      "Train roc_auc: 0.6971765095740495\n",
      "Train pr_auc: 0.687608227563087\n",
      "Train average_precision: 0.6639685847950041\n",
      "Val loss: 1.2457727996748447\n",
      "Val accuracy: 0.447854520422212\n",
      "Val precision: 0.47171572593191863\n",
      "Val recall: 0.8696649839375861\n",
      "Val f1: 0.61166027839419\n",
      "Val roc_auc: 0.4225265022458736\n",
      "Val pr_auc: 0.47782948429123806\n",
      "Val average_precision: 0.4778991467296386\n",
      "Best val_f1: 0.61166027839419\n",
      "\n",
      "Epoch 2\n",
      "Early stopping counter: 1\n",
      "Epoch Number: 2\n",
      "Epoch time: 10.18732738494873\n",
      "Train loss: 1.1962706754332468\n",
      "Train accuracy: 0.6812261134076905\n",
      "Train precision: 0.6240987021310688\n",
      "Train recall: 0.9113953669760549\n",
      "Train f1: 0.7408698960182603\n",
      "Train roc_auc: 0.7456062402215877\n",
      "Train pr_auc: 0.69300839604029\n",
      "Train average_precision: 0.6900671400854346\n",
      "Val loss: 1.0455128691353641\n",
      "Val accuracy: 0.40953418999541075\n",
      "Val precision: 0.44763233047751877\n",
      "Val recall: 0.7732905002294631\n",
      "Val f1: 0.567029823749632\n",
      "Val roc_auc: 0.3892911439686389\n",
      "Val pr_auc: 0.4484878396777409\n",
      "Val average_precision: 0.4485670054267912\n",
      "Best val_f1: 0.61166027839419\n",
      "\n",
      "Epoch 3\n",
      "Early stopping counter: 2\n",
      "Epoch Number: 3\n",
      "Epoch time: 10.293035745620728\n",
      "Train loss: 0.6424015488410532\n",
      "Train accuracy: 0.6826105607986896\n",
      "Train precision: 0.616872582054162\n",
      "Train recall: 0.9638483737617971\n",
      "Train f1: 0.7522790570258876\n",
      "Train roc_auc: 0.84047478321633\n",
      "Train pr_auc: 0.7734766968091592\n",
      "Train average_precision: 0.7735068189193\n",
      "Val loss: 1.2620654100136006\n",
      "Val accuracy: 0.39656952730610373\n",
      "Val precision: 0.43894344734168644\n",
      "Val recall: 0.7435750344194585\n",
      "Val f1: 0.5520207827605298\n",
      "Val roc_auc: 0.38552985589214583\n",
      "Val pr_auc: 0.43956716709284144\n",
      "Val average_precision: 0.43965354793412786\n",
      "Best val_f1: 0.61166027839419\n",
      "\n",
      "Epoch 4\n",
      "Early stopping counter: 3\n",
      "Epoch Number: 4\n",
      "Epoch time: 9.630484580993652\n",
      "Train loss: 0.626253742380927\n",
      "Train accuracy: 0.6801536541611419\n",
      "Train precision: 0.614995892559309\n",
      "Train recall: 0.9634583885812339\n",
      "Train f1: 0.7507635269627582\n",
      "Train roc_auc: 0.8687175116656852\n",
      "Train pr_auc: 0.8241242680406156\n",
      "Train average_precision: 0.8241548776528824\n",
      "Val loss: 1.328452283973659\n",
      "Val accuracy: 0.39622533272143184\n",
      "Val precision: 0.43870705427932505\n",
      "Val recall: 0.7427719137218908\n",
      "Val f1: 0.5516124909470455\n",
      "Val roc_auc: 0.3809388835311237\n",
      "Val pr_auc: 0.43788474603977445\n",
      "Val average_precision: 0.43797016156712487\n",
      "Best val_f1: 0.61166027839419\n",
      "\n",
      "Epoch 5\n",
      "Early stopping counter: 4\n",
      "Epoch Number: 5\n",
      "Epoch time: 9.485216856002808\n",
      "Train loss: 0.6199836696026628\n",
      "Train accuracy: 0.6776577490055378\n",
      "Train precision: 0.6132054372406253\n",
      "Train recall: 0.9623274315576008\n",
      "Train f1: 0.7490855000531244\n",
      "Train roc_auc: 0.8783419723927723\n",
      "Train pr_auc: 0.848046210502555\n",
      "Train average_precision: 0.8480784824022829\n",
      "Val loss: 1.2315361525986221\n",
      "Val accuracy: 0.39605323542909593\n",
      "Val precision: 0.43859292395282634\n",
      "Val recall: 0.7424277191372189\n",
      "Val f1: 0.5514273540690242\n",
      "Val roc_auc: 0.4005105474026025\n",
      "Val pr_auc: 0.4543709019887264\n",
      "Val average_precision: 0.45445704232620826\n",
      "Best val_f1: 0.61166027839419\n",
      "\n",
      "Epoch 6\n",
      "Early stopping counter: 5\n",
      "Early stopping triggered!\n",
      "Epoch Number: 6\n",
      "Epoch time: 10.859920263290405\n",
      "Train loss: 0.6121286543230167\n",
      "Train accuracy: 0.6827080570938304\n",
      "Train precision: 0.6173420828532785\n",
      "Train recall: 0.9612354730520241\n",
      "Train f1: 0.7518301610541728\n",
      "Train roc_auc: 0.8872360458718449\n",
      "Train pr_auc: 0.8657994264071925\n",
      "Train average_precision: 0.8658342618069833\n",
      "Val loss: 1.173897847151145\n",
      "Val accuracy: 0.3962826984855438\n",
      "Val precision: 0.43874508741021817\n",
      "Val recall: 0.7428866452501147\n",
      "Val f1: 0.5516741927238646\n",
      "Val roc_auc: 0.41984485022349216\n",
      "Val pr_auc: 0.4740572634718498\n",
      "Val average_precision: 0.4741365992182913\n",
      "Best val_f1: 0.61166027839419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [00:05<00:00, 234.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC: 0.3420\n",
      "Test AP: 0.4871\n",
      "Test F1: 0.5163\n",
      "Test Accuracy: 0.3766\n",
      "Test Precission: 0.4218\n",
      "Test Recall: 0.6655\n",
      "Test MCC: -0.3023\n",
      "Test MR: 9.8770\n",
      "Fold 2\n",
      "Device: 'cuda'\n",
      "Total Number of Parameters: 249984\n",
      "Total Number of Trainable Parameters: 249984\n",
      "Starting training loop at 2024-08-05 16:13:58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948e9518e0e74b09b5a41564779c19a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MiniBatches:   0%|          | 0/6740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "New best model saved!\n",
      "Epoch Number: 1\n",
      "Epoch time: 10.759796142578125\n",
      "Train loss: 2.8548794336747054\n",
      "Train accuracy: 0.6154356134466891\n",
      "Train precision: 0.5787862656374767\n",
      "Train recall: 0.8480227751345449\n",
      "Train f1: 0.6880022780484718\n",
      "Train roc_auc: 0.6943458668849813\n",
      "Train pr_auc: 0.6842439714341574\n",
      "Train average_precision: 0.6589796954367474\n",
      "Val loss: 1.740153941206443\n",
      "Val accuracy: 0.4471661312528683\n",
      "Val precision: 0.47142768505304955\n",
      "Val recall: 0.8717301514456173\n",
      "Val f1: 0.6119276768815688\n",
      "Val roc_auc: 0.3954311170122583\n",
      "Val pr_auc: 0.45429998211703115\n",
      "Val average_precision: 0.4543304756151678\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n",
      "Epoch 2\n",
      "Early stopping counter: 1\n",
      "Epoch Number: 2\n",
      "Epoch time: 11.508731365203857\n",
      "Train loss: 1.2372667997555244\n",
      "Train accuracy: 0.6718274705561189\n",
      "Train precision: 0.6154007333682556\n",
      "Train recall: 0.9163091802511505\n",
      "Train f1: 0.7362978283350569\n",
      "Train roc_auc: 0.7291389197782203\n",
      "Train pr_auc: 0.674632308321145\n",
      "Train average_precision: 0.6713832763348493\n",
      "Val loss: 1.5060934855606094\n",
      "Val accuracy: 0.4027076640660854\n",
      "Val precision: 0.44325481798715205\n",
      "Val recall: 0.7599816429554842\n",
      "Val f1: 0.5599323753169907\n",
      "Val roc_auc: 0.3715160434061121\n",
      "Val pr_auc: 0.4327529840339607\n",
      "Val average_precision: 0.43283780082547485\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n",
      "Epoch 3\n",
      "Early stopping counter: 2\n",
      "Epoch Number: 3\n",
      "Epoch time: 12.32430911064148\n",
      "Train loss: 0.6432397856082107\n",
      "Train accuracy: 0.6744988690429764\n",
      "Train precision: 0.610478753611022\n",
      "Train recall: 0.9642383589423602\n",
      "Train f1: 0.747622575329211\n",
      "Train roc_auc: 0.8503485146169859\n",
      "Train pr_auc: 0.7892891887449032\n",
      "Train average_precision: 0.7893205834931121\n",
      "Val loss: 1.3852415954346184\n",
      "Val accuracy: 0.3953074804956402\n",
      "Val precision: 0.4382151804455278\n",
      "Val recall: 0.7425424506654429\n",
      "Val f1: 0.5511603150947414\n",
      "Val roc_auc: 0.38505514037262945\n",
      "Val pr_auc: 0.44052105651159895\n",
      "Val average_precision: 0.4406071221362951\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n",
      "Epoch 4\n",
      "Early stopping counter: 3\n",
      "Epoch Number: 4\n",
      "Epoch time: 12.087929248809814\n",
      "Train loss: 0.6267609346537222\n",
      "Train accuracy: 0.6745963653381172\n",
      "Train precision: 0.6107017457099055\n",
      "Train recall: 0.9631853989548397\n",
      "Train f1: 0.7474729132619091\n",
      "Train roc_auc: 0.8714506284023666\n",
      "Train pr_auc: 0.8339920344086136\n",
      "Train average_precision: 0.8340242747586378\n",
      "Val loss: 1.1004488269488018\n",
      "Val accuracy: 0.395766406608536\n",
      "Val precision: 0.4385359583248765\n",
      "Val recall: 0.7436897659476824\n",
      "Val f1: 0.5517300080861387\n",
      "Val roc_auc: 0.40735408692764025\n",
      "Val pr_auc: 0.46133985186103166\n",
      "Val average_precision: 0.4614277237867329\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n",
      "Epoch 5\n",
      "Early stopping counter: 4\n",
      "Epoch Number: 5\n",
      "Epoch time: 11.248342990875244\n",
      "Train loss: 0.6206720114646113\n",
      "Train accuracy: 0.6733094142422588\n",
      "Train precision: 0.6097988832336809\n",
      "Train recall: 0.9625224241478824\n",
      "Train f1: 0.7465968903139937\n",
      "Train roc_auc: 0.8854808213099461\n",
      "Train pr_auc: 0.8649086893633703\n",
      "Train average_precision: 0.864940991450396\n",
      "Val loss: 1.1650827248127034\n",
      "Val accuracy: 0.39570904084442404\n",
      "Val precision: 0.4384646628757108\n",
      "Val recall: 0.7431161083065626\n",
      "Val f1: 0.5515156675749319\n",
      "Val roc_auc: 0.42870592577022815\n",
      "Val pr_auc: 0.4812733980313451\n",
      "Val average_precision: 0.4813696176553366\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n",
      "Epoch 6\n",
      "Early stopping counter: 5\n",
      "Early stopping triggered!\n",
      "Epoch Number: 6\n",
      "Epoch time: 11.744775772094727\n",
      "Train loss: 0.6137331519043654\n",
      "Train accuracy: 0.6738943920131035\n",
      "Train precision: 0.610158604674144\n",
      "Train recall: 0.9631853989548397\n",
      "Train f1: 0.7470659407138536\n",
      "Train roc_auc: 0.8917222564113658\n",
      "Train pr_auc: 0.878092811692358\n",
      "Train average_precision: 0.8781280219847404\n",
      "Val loss: 1.3158538079523778\n",
      "Val accuracy: 0.395766406608536\n",
      "Val precision: 0.4385026737967914\n",
      "Val recall: 0.7432308398347865\n",
      "Val f1: 0.5515773340712673\n",
      "Val roc_auc: 0.4583795256296228\n",
      "Val pr_auc: 0.512482129756922\n",
      "Val average_precision: 0.5125850403349885\n",
      "Best val_f1: 0.6119276768815688\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [00:06<00:00, 209.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC: 0.3067\n",
      "Test AP: 0.4599\n",
      "Test F1: 0.5245\n",
      "Test Accuracy: 0.3810\n",
      "Test Precission: 0.4258\n",
      "Test Recall: 0.6829\n",
      "Test MCC: -0.2986\n",
      "Test MR: 9.6716\n",
      "Fold 3\n",
      "Device: 'cuda'\n",
      "Total Number of Parameters: 249984\n",
      "Total Number of Trainable Parameters: 249984\n",
      "Starting training loop at 2024-08-05 16:15:14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d503047fc6a64b85a3c17746cb57721c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MiniBatches:   0%|          | 0/6740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "New best model saved!\n",
      "Epoch Number: 1\n",
      "Epoch time: 10.446845054626465\n",
      "Train loss: 2.7653167259663416\n",
      "Train accuracy: 0.616683566024491\n",
      "Train precision: 0.5798931909212283\n",
      "Train recall: 0.8469308166289681\n",
      "Train f1: 0.6884232549293096\n",
      "Train roc_auc: 0.69648837553673\n",
      "Train pr_auc: 0.6839607214744425\n",
      "Train average_precision: 0.6601892782516559\n",
      "Val loss: 1.404423611340942\n",
      "Val accuracy: 0.453132170720514\n",
      "Val precision: 0.4746131377788826\n",
      "Val recall: 0.8762046810463515\n",
      "Val f1: 0.6157133067279398\n",
      "Val roc_auc: 0.4152265902927123\n",
      "Val pr_auc: 0.4655298276074895\n",
      "Val average_precision: 0.4656009232402104\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n",
      "Epoch 2\n",
      "Early stopping counter: 1\n",
      "Epoch Number: 2\n",
      "Epoch time: 11.460914850234985\n",
      "Train loss: 1.382160448225359\n",
      "Train accuracy: 0.6802706497153108\n",
      "Train precision: 0.6250169033130494\n",
      "Train recall: 0.9012557522814133\n",
      "Train f1: 0.7381382052797164\n",
      "Train roc_auc: 0.7391693600667744\n",
      "Train pr_auc: 0.6885370844276095\n",
      "Train average_precision: 0.6845510138536021\n",
      "Val loss: 1.1277756948610802\n",
      "Val accuracy: 0.42008949059201467\n",
      "Val precision: 0.454198724271717\n",
      "Val recall: 0.7924506654428637\n",
      "Val f1: 0.5774359403084897\n",
      "Val roc_auc: 0.3991434375250762\n",
      "Val pr_auc: 0.4504526456741519\n",
      "Val average_precision: 0.45052644751078746\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n",
      "Epoch 3\n",
      "Early stopping counter: 2\n",
      "Epoch Number: 3\n",
      "Epoch time: 11.597789764404297\n",
      "Train loss: 0.6430558471608341\n",
      "Train accuracy: 0.6818890882146479\n",
      "Train precision: 0.6162280701754386\n",
      "Train recall: 0.9643553544965291\n",
      "Train f1: 0.7519537783183823\n",
      "Train roc_auc: 0.8413133030646107\n",
      "Train pr_auc: 0.7750409564058146\n",
      "Train average_precision: 0.7750708468147091\n",
      "Val loss: 1.253986343850583\n",
      "Val accuracy: 0.4138366223038091\n",
      "Val precision: 0.45003991484832356\n",
      "Val recall: 0.7761587884350619\n",
      "Val f1: 0.5697321879737242\n",
      "Val roc_auc: 0.38500827235906343\n",
      "Val pr_auc: 0.43895063174833376\n",
      "Val average_precision: 0.43903161513500283\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n",
      "Epoch 4\n",
      "Early stopping counter: 3\n",
      "Epoch Number: 4\n",
      "Epoch time: 11.580726385116577\n",
      "Train loss: 0.6295271847313479\n",
      "Train accuracy: 0.6734654083144841\n",
      "Train precision: 0.6098488590338832\n",
      "Train recall: 0.9630294048826145\n",
      "Train f1: 0.7467868267456981\n",
      "Train roc_auc: 0.8705548442142585\n",
      "Train pr_auc: 0.8341357023215428\n",
      "Train average_precision: 0.8341669371371857\n",
      "Val loss: 1.1600040118772905\n",
      "Val accuracy: 0.4150986691142726\n",
      "Val precision: 0.45086321381142097\n",
      "Val recall: 0.7790270766406608\n",
      "Val f1: 0.571164199192463\n",
      "Val roc_auc: 0.4131835898215706\n",
      "Val pr_auc: 0.4591919545745463\n",
      "Val average_precision: 0.4592705954893514\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n",
      "Epoch 5\n",
      "Early stopping counter: 4\n",
      "Epoch Number: 5\n",
      "Epoch time: 11.624252319335938\n",
      "Train loss: 0.6167469370989431\n",
      "Train accuracy: 0.6762148038374541\n",
      "Train precision: 0.61212437033177\n",
      "Train recall: 0.9620154434131503\n",
      "Train f1: 0.7481839826511579\n",
      "Train roc_auc: 0.8857800507550763\n",
      "Train pr_auc: 0.8622551056892738\n",
      "Train average_precision: 0.8622868460204112\n",
      "Val loss: 1.0819460257287428\n",
      "Val accuracy: 0.41681964203763194\n",
      "Val precision: 0.4519803947542721\n",
      "Val recall: 0.7829279486002754\n",
      "Val f1: 0.5731082556479382\n",
      "Val roc_auc: 0.4435376676928443\n",
      "Val pr_auc: 0.4877545086305074\n",
      "Val average_precision: 0.48782858247439914\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n",
      "Epoch 6\n",
      "Early stopping counter: 5\n",
      "Early stopping triggered!\n",
      "Epoch Number: 6\n",
      "Epoch time: 11.904126644134521\n",
      "Train loss: 0.6164301209467605\n",
      "Train accuracy: 0.6747328601513143\n",
      "Train precision: 0.610928177069149\n",
      "Train recall: 0.9623274315576008\n",
      "Train f1: 0.7473838838156679\n",
      "Train roc_auc: 0.8880648792073145\n",
      "Train pr_auc: 0.8686853200711864\n",
      "Train average_precision: 0.8687188622128522\n",
      "Val loss: 1.1919591218888104\n",
      "Val accuracy: 0.4134350619550252\n",
      "Val precision: 0.449763632731873\n",
      "Val recall: 0.7750114731528224\n",
      "Val f1: 0.5692016010111649\n",
      "Val roc_auc: 0.4385578902435763\n",
      "Val pr_auc: 0.47853032819013797\n",
      "Val average_precision: 0.47861018939975625\n",
      "Best val_f1: 0.6157133067279398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [00:07<00:00, 194.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test AUC: 0.3172\n",
      "Test AP: 0.4696\n",
      "Test F1: 0.4944\n",
      "Test Accuracy: 0.3610\n",
      "Test Precission: 0.4090\n",
      "Test Recall: 0.6248\n",
      "Test MCC: -0.3272\n",
      "Test MR: 9.0816\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = train_wrap_cv(homo_data, \"sdv-hgnn-variant-homo\", cv_fold=3, shuffle=True, \n",
    "                             num_neighbors=[10, 4], batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.321979</td>\n",
       "      <td>0.014771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ap</td>\n",
       "      <td>0.472182</td>\n",
       "      <td>0.011243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.511760</td>\n",
       "      <td>0.012723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.008572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.418875</td>\n",
       "      <td>0.007154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.657741</td>\n",
       "      <td>0.024335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mr</td>\n",
       "      <td>9.543407</td>\n",
       "      <td>0.337145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mcc</td>\n",
       "      <td>-0.309364</td>\n",
       "      <td>0.012701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric      Mean  Standard Deviation\n",
       "0        auc  0.321979            0.014771\n",
       "1         ap  0.472182            0.011243\n",
       "2         f1  0.511760            0.012723\n",
       "3        acc  0.372881            0.008572\n",
       "4  precision  0.418875            0.007154\n",
       "5     recall  0.657741            0.024335\n",
       "6         mr  9.543407            0.337145\n",
       "7        mcc -0.309364            0.012701"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['auc', 'ap', 'f1', 'acc', 'precision', 'recall', 'mr', 'mcc']\n",
    "metrics_mean_value = np.mean(eval_metrics, axis=0)\n",
    "metrics_std = np.std(eval_metrics, axis=0)\n",
    "df = pd.DataFrame({\n",
    "    'Metric': metrics,\n",
    "    'Mean': metrics_mean_value,\n",
    "    'Standard Deviation': metrics_std\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
